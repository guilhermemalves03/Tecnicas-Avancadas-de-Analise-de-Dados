{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a04407",
   "metadata": {},
   "source": [
    "# **Advanced Techniques in Data Analysis - MACHINE LEARNING PROJECT** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da799cb",
   "metadata": {},
   "source": [
    "## Objectives and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf2c8d",
   "metadata": {},
   "source": [
    "In this project we want to make a deep dive into the world of stock prediction, more specifically on the PayPal stock. Our objectives are:\n",
    "\n",
    "- Make a rigorous time series analysis \n",
    "\n",
    "- Make a classifier to evaluate if the stock will go up or down\n",
    "\n",
    "- Risk management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224516a9",
   "metadata": {},
   "source": [
    "# Importing the Paypal Stock information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f26ac",
   "metadata": {},
   "source": [
    "## Imports and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff00650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import sklearn.svm as svc\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f97f95",
   "metadata": {},
   "source": [
    "Now we download the Standard & Poor 500 dataset (or keep it updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d592c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"yash16jr/s-and-p500-daily-update-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f53eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "path = \"/Users/guilhermealves/.cache/kagglehub/datasets/yash16jr/s-and-p500-daily-update-dataset/versions/275\"\n",
    "\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\n",
    "    \n",
    "    '/Users/guilhermealves/.cache/kagglehub/datasets/yash16jr/s-and-p500-daily-update-dataset/versions/275/SnP_daily_update.csv',\n",
    "    \n",
    "    header=[0, 1],         \n",
    "    index_col=0,          \n",
    "    parse_dates=True      \n",
    ")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978eca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pypl_completo = df.xs('PYPL', level=1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pypl_completo.columns = [col.lower() for col in df_pypl_completo.columns]\n",
    "\n",
    "if 'price' in df_pypl_completo.columns:\n",
    "    df_pypl_completo = df_pypl_completo.drop(columns=['price'])\n",
    "\n",
    "df_pypl_completo.dropna(subset=['close'], inplace=True)\n",
    "\n",
    "df_pypl = df_pypl_completo.copy()\n",
    "\n",
    "print(\"PayPal's DataFrame (PYPL) Clean and Ready:\")\n",
    "print(df_pypl.head())\n",
    "print(f\"Column number: {df_pypl.shape[1]}\")\n",
    "print(f\"Existent Columns: {list(df_pypl.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390b50a",
   "metadata": {},
   "source": [
    "# Time Series Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403358ab",
   "metadata": {},
   "source": [
    "Stock market predicition must be preceded by a rigorous Time Series Analysis. Financial data is unique because observations are not independent and identifcally distributed, they exibit autocorrelation and non-stationarity.\n",
    "\n",
    "\n",
    "Time Series Analysis serves two critical functions:\n",
    "\n",
    "- Risk Management: Quantifying extreme events. This will provide context for interpreting machine learning models.\n",
    "\n",
    "- Validation and feature engineering: It allow us to confirm the statistical properties of data (Stationarity and other)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4693f69",
   "metadata": {},
   "source": [
    "## Discrete Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce04c0",
   "metadata": {},
   "source": [
    "### Logarithmic Return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec8909",
   "metadata": {},
   "source": [
    "### Histogram of returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa419b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = 100 * (df_pypl['close'].pct_change())\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(ret, bins=50, density = True, label = \"Daily Returns (PayPal)\", color = \"skyblue\", alpha=0.7)\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu, sigma = ret.mean(), ret.std()\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, sigma) \n",
    "\n",
    "\n",
    "plt.plot(x, p, 'k', linewidth=2, label='Normal Curve')\n",
    "\n",
    "\n",
    "title = \"Histograma de Retornos Diários da PayPal vs. Distribuição Normal\"\n",
    "plt.title(title, fontsize=15)\n",
    "plt.xlabel(\"Retornos Diários (%)\", fontsize=12)\n",
    "plt.ylabel(\"Frequência Normalizada\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9631db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import jarque_bera, skew, kurtosis\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ret_clean = ret.dropna() \n",
    "\n",
    "\n",
    "jb_result = jarque_bera(ret_clean)\n",
    "jb_statistic = jb_result[0]\n",
    "jb_pvalue = jb_result[1]\n",
    "\n",
    "\n",
    "skewness_value = skew(ret_clean)\n",
    "\n",
    "kurtosis_value = kurtosis(ret_clean, fisher=False) \n",
    "\n",
    "print(f\"Estatística Jarque-Bera: {jb_statistic:.4f}\")\n",
    "print(f\"P-value: {jb_pvalue:.4f}\")\n",
    "print(f\"Assimetria (Skewness): {skewness_value:.4f}\")\n",
    "print(f\"Curtose (Kurtosis): {kurtosis_value:.4f}\")\n",
    "\n",
    "# Regra de Decisão\n",
    "if jb_pvalue <= 0.05:\n",
    "    print(\"Conclusão: Rejeitamos a Hipótese Nula (H0). Os retornos NÃO são normalmente distribuídos (devido a caudas gordas ou assimetria).\")\n",
    "else:\n",
    "    print(\"Conclusão: Não rejeitamos H0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f2eac",
   "metadata": {},
   "source": [
    "## Stationary Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10b52f",
   "metadata": {},
   "source": [
    "## Time Dependency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7455f8c",
   "metadata": {},
   "source": [
    "# PayPal Stock Prediction using a Support Vector Machine \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91c911",
   "metadata": {},
   "source": [
    "The idea of Support Vector Machine is by finding a hyperplane to divide the data into groups, this will classify if the stock is going up or down based on historic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e14521",
   "metadata": {},
   "source": [
    "This only shows history up to 2015-07-06 because PayPal only turned public in July of 2015 (The S&P500 file starts at 2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f58009",
   "metadata": {},
   "source": [
    "Since the Support Vector Machine is a classifier we need to define the target\n",
    "\n",
    "- 1 if tomorrow's price is BIGGER than today\n",
    "- 0 if tomorrow's price is SMALLER than today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fceac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pypl[\"Target\"]= np.where(df_pypl[\"close\"].shift(-1) > df_pypl[\"close\"], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c05be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pypl[\"SMA_20\"]= df_pypl[\"close\"].rolling(window=20).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pypl[\"SMA_50\"]=df_pypl[\"close\"].rolling(window=50).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_final = df_pypl.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"open\", \"high\", \"low\", \"close\", \"volume\", \"SMA_20\", \"SMA_50\"]\n",
    "X = df_ml_final [features]\n",
    "y = df_ml_final[\"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff26d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"DataFrame da Paypal com features e Target\")\n",
    "print(df_ml_final[features + [\"Target\"]].tail())\n",
    "print(f\"\\nNúmero de amostras prontas para o SVM (X e y): {X.shape[0]}\")\n",
    "print(f\"Número de Features (X): {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0640c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7babc8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "num_samples= X.shape[0]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled= scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c014047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC (kernel= \"rbf\", C=1.0, gamma = \"scale\", random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d654175",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm= confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['DOWN (0)', 'UP (1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb891c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "532320be",
   "metadata": {},
   "source": [
    "# Risk Management on PayPal Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5963506c",
   "metadata": {},
   "source": [
    "Risk Management is fundamental to predict the behavior of stocks. Markowitz stated in his famous book, that volatility, which was a phenomenon not closely linked to math, to be the standard deviation of returns, $\\sigma_r$.\n",
    "\n",
    "There are Several methods to predict volatility, we will start with 2 classical ones and then move to a machine learning approach, with support vector regressors and neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b7729",
   "metadata": {},
   "source": [
    "But first we will calculate the return volatility by using the formula for the Realized Volatility:\n",
    "\n",
    "$$ \\sigma=\\sqrt{\\frac{1}{n-1}\\sum_{n=1}^N (r_n -\\mu_r)^2} $$\n",
    "\n",
    "where $N$ is the number of observations, $r_n$ is the return at observation $n$ and $\\mu_r$ is the average return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = 100 * (df_pypl['close'].pct_change()).dropna()\n",
    "realized_vol = ret.rolling(5).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(realized_vol, label='Volatilidade Realizada (5 dias)', color='blue')\n",
    "plt.title('Volatilidade Realizada da PayPal (PYPL)')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Volatilidade (%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16d36e",
   "metadata": {},
   "source": [
    "This is interesting because we can see big spikes on 2020 and 2022:\n",
    "\n",
    "- 2020: Begining of the COVID pandemic\n",
    "\n",
    "- 2022: A shocking report about the Q4 2021 was released. The sharp drop was due to the company drastically cutting its growth forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "retv = ret.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db467ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_pypl.index[1:], retv, label='Retornos Diários (%)', color='green')\n",
    "plt.title('Volatility clustering of PayPal (PYPL)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Returns (%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f50030e",
   "metadata": {},
   "source": [
    "with this volatility clustering we can now in which direction was the spikes on 2020 and 2022 (downwards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cece65",
   "metadata": {},
   "source": [
    "## ARCH Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d179340",
   "metadata": {},
   "source": [
    "The ARCH model was one of the first statistical models introduced to predict volatility: the ARCH model is a univariate model and based on historical asset returns\n",
    "\n",
    "$$\\sigma_t ^2 = \\omega + \\sum_{k=1}^p \\alpha_k (r_{t - k})^2$$\n",
    "\n",
    "where the mean model is:\n",
    "\n",
    "$$ r_t = \\sigma_t \\epsilon_t $$\n",
    "\n",
    "where $\\epsilon_t$ is assumed to be normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb446134",
   "metadata": {},
   "source": [
    "In this project we will not implement the ARCH model from the ground, instead we will use the arch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60796fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d89485",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = arch_model(ret, mean=\"Zero\", vol=\"ARCH\", p=1).fit(disp=\"off\")\n",
    "print(arch.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=252\n",
    "split_date=ret.iloc[-n:].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_arch = []\n",
    "for p in range(1, 5):\n",
    "    arch = arch_model(ret, mean='zero', vol='ARCH', p=p).fit(disp='off')\n",
    "    bic_arch.append(arch.bic)\n",
    "    if arch.bic == np.min(bic_arch):\n",
    "        best_param = p\n",
    "arch = arch_model(ret, mean='zero', vol='ARCH', p=best_param).fit(disp='off')\n",
    "print(arch.summary())\n",
    "\n",
    "forecast_arch = arch.forecast(start=split_date[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b8884",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_arch=np.sqrt(mse(realized_vol[-n:] /100 , np.sqrt(forecast_arch.variance.iloc[-len(split_date): ] /100 )))\n",
    "print(f\"RMSE for ARCH model: {rmse_arch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(realized_vol /100 , label= \"Realized Volatility\")\n",
    "plt.plot(np.sqrt(forecast_arch.variance.iloc[-len(split_date):]) /100 , label = \"Volatility Prediction - ARCH\")\n",
    "plt.title(\"Volatility Prediction with ARCH\", fontsize = 12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8fc2f6",
   "metadata": {},
   "source": [
    "As we can see we got a nice prediction of the volatility, even using an old model such as ARCH.\n",
    "\n",
    "Disadvantages of using ARCH:\n",
    "- Needs a lot of parameters: Markets are complex, and ARCH model can not capture all the shocks of volatility in data using a small $p$. To get a good modelation, a higher $p$ would be necessary\n",
    "\n",
    "- Non-negativity: Two of the assumptions of ARCH are that $\\alpha_k$ and $\\omega$ are $>0$ which turns the volatility hard to model.\n",
    "\n",
    "- Information assimetry: ARCH only looks for past returns, $r_{t-k} ^2$ to predict future volatility, $\\sigma_t ^2$, but ignores the direction of the shock. This means that the model cannot capture the leverage effect: Bad news tend to a higher volatility and good news tends to lower volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3dad0",
   "metadata": {},
   "source": [
    "## GARCH model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e4003",
   "metadata": {},
   "source": [
    "GARCH is an extension of ARCH incorporating lagged conditional variance. This makes the model multivariate in the sense that it is an autoregressive moving average model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_garc= []\n",
    "\n",
    "for p in range (1,5):\n",
    "    for q in range(1,5):\n",
    "        garch = arch_model(ret, mean=\"zero\", vol=\"GARCH\", p=p, o=0, q=q).fit(disp=\"off\")\n",
    "        bic_garc.append(garch.bic)\n",
    "        if garch.bic == np.min(bic_garc):\n",
    "            best_param = (p,q)\n",
    "garch = arch_model(ret, mean=\"zero\", vol=\"GARCH\", p=best_param[0], o=0, q=best_param[1]).fit(disp=\"off\")\n",
    "print(garch.summary())\n",
    "forecast_garch = garch.forecast(start=split_date[0])\n",
    "forecast_garch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_garch = np.sqrt(mse(realized_vol[-n:] /100 , np.sqrt(forecast_garch.variance.iloc[-len(split_date): ] /100 )))\n",
    "print(f\"RMSE for GARCH model: {rmse_garch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40365d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(realized_vol /100 , label= \"Realized Volatility\")\n",
    "plt.plot(np.sqrt(forecast_garch.variance.iloc[-len(split_date):]) /100 , label = \"Volatility Prediction - GARCH\")\n",
    "plt.title(\"Volatility Prediction with GARCH\", fontsize = 12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0cd4b5",
   "metadata": {},
   "source": [
    "We can see that GARCH got a slight better value than ARCH, but still a bad value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f702d0",
   "metadata": {},
   "source": [
    "## Support Vector Regression with GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b5cac",
   "metadata": {},
   "source": [
    "In this section we complement the volatility estimates generated by the GARCH model with Support Vector Regression (SVR).. The goal is to allow a non-linear model to learn additional patterns that GARCH cannot capture. SVR is particularly suitable for financial volatility because it can model complex relationships while remaining robust to noise, a common characteristic of financial time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f4256",
   "metadata": {},
   "source": [
    "### Theoretical Background and Mechanism of SVR\n",
    "\n",
    "Support Vector Regression (SVR) is an extension of Support Vector Machines (SVM) for regression tasks.  \n",
    "SVR tries to find a function \\(f(x) = w^T \\phi(x) + b\\) that approximates the target \\(y\\) with a maximum deviation of \\(\\epsilon\\).  \n",
    "\n",
    "It solves the following optimization problem:\n",
    "\n",
    "$\\min_{w,b} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)$\n",
    "\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\begin{cases} \n",
    "y_i - w^T \\phi(x_i) - b \\leq \\epsilon + \\xi_i \\\\\n",
    "w^T \\phi(x_i) + b - y_i \\leq \\epsilon + \\xi_i^* \\\\\n",
    "\\xi_i, \\xi_i^* \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Here, $\\xi_i, \\xi_i^*$ are slack variables for points outside the $\\epsilon$-insensitive tube, $C$ is a regularization parameter controlling the trade-off between flatness and tolerance to deviations, and $\\phi(x)$ is a (possibly nonlinear) feature mapping.\n",
    "\n",
    "In essence, SVR minimizes both the model complexity $(\\|w\\|^2)$ and the error outside the $\\epsilon$-tube, making it robust to noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82d77a",
   "metadata": {},
   "source": [
    "### Why SVR is Useful with GARCH\n",
    "\n",
    "GARCH models capture linear autoregressive patterns and volatility clustering but may miss nonlinear effects.  \n",
    "SVR complements GARCH by learning residual volatility patterns not captured by the linear GARCH structure.  \n",
    "\n",
    "Using kernels (linear, polynomial, RBF), SVR can model a variety of nonlinear relationships in volatility,\n",
    "providing more flexible and robust predictions when combined with GARCH estimates.  \n",
    "\n",
    "This hybrid approach leverages the strengths of both models: GARCH captures the main volatility dynamics,\n",
    "and SVR refines the prediction by modeling residual nonlinearities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25bf76e",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18341c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc81158",
   "metadata": {},
   "source": [
    "We use the realized volatility estimated from the rolling standard deviation of returns as the target variable for the SVR. This allows the SVR to learn from the actual volatility observed in the data, rather than relying solely on model-based estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e4b37e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ret' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m realized_vol \u001b[38;5;241m=\u001b[39m ret\u001b[38;5;241m.\u001b[39mrolling(\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mstd()\n\u001b[0;32m      2\u001b[0m realized_vol \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(realized_vol)\n\u001b[0;32m      3\u001b[0m realized_vol\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ret' is not defined"
     ]
    }
   ],
   "source": [
    "realized_vol = ret.rolling(5).std()\n",
    "realized_vol = pd.DataFrame(realized_vol)\n",
    "realized_vol.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad167c2",
   "metadata": {},
   "source": [
    "Input features are constructed using lagged squared returns. This captures volatility clustering and persistence, which are key characteristics of financial time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33478cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_svm = ret ** 2\n",
    "returns_svm = returns_svm.reset_index()\n",
    "del returns_svm[\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69bb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([realized_vol, returns_svm], axis=1, ignore_index=True)\n",
    "X = X[4:].copy()\n",
    "X = X.reset_index()\n",
    "X.drop(\"index\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106164d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "realized_vol = realized_vol.dropna().reset_index()\n",
    "realized_vol.drop(\"index\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd20a0c",
   "metadata": {},
   "source": [
    "We configure SVR models with multiple kernel functions: linear, polynomial, and radial basis function (RBF). Each kernel captures different types of relationships in the data. Linear kernel captures simple trends, polynomial kernel captures interactions between features, and RBF kernel captures more complex nonlinear patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0245ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_poly=SVR(kernel=\"poly\", degree=2)\n",
    "svr_lin=SVR(kernel=\"linear\")\n",
    "svr_rbf=SVR(kernel=\"rbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6fa6e",
   "metadata": {},
   "source": [
    "Hyperparameters are tuned using randomized cross-validation. This method efficiently searches through the hyperparameter space to find a balance between model complexity and predictive accuracy, reducing the risk of overfitting while improving out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4543775",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_grid = {'gamma': sp_rand(), 'C': sp_rand(), 'epsilon': sp_rand()}\n",
    "clf_lin = RandomizedSearchCV(svr_lin, para_grid, n_jobs=-1)\n",
    "clf_lin.fit(X.iloc[:-n].values, realized_vol.iloc[1:-(n-1)].values.reshape(-1,))\n",
    "predict_svr_lin = clf_lin.predict(X.iloc[-n:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b76e1",
   "metadata": {},
   "source": [
    "After training, the SVR model is used to predict out-of-sample volatility. Comparing these predictions with realized volatility allows us to assess the model's ability to capture real-world volatility fluctuations beyond what the GARCH model captures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeabf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_svr_lin = pd.DataFrame(predict_svr_lin)\n",
    "predict_svr_lin.index = ret.iloc[-n:].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd11e6",
   "metadata": {},
   "source": [
    "We compute the root mean squared error (RMSE) to quantify the predictive performance of the SVR model. Lower RMSE indicates better alignment with realized volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b355f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_svr = np.sqrt(mse(realized_vol.iloc[-n:] / 100, predict_svr_lin / 100))\n",
    "print(f\"The SVR model with a linear kernel achieved an RMSE of {rmse_svr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa255ec1",
   "metadata": {},
   "source": [
    "Plotting the realized volatility against the SVR predictions provides a visual assessment of how well the model captures volatility patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a02187",
   "metadata": {},
   "outputs": [],
   "source": [
    "realized_vol.index=ret.iloc[4:].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ca680",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(realized_vol / 100, label=\"Realized Volatility\")\n",
    "plt.plot(predict_svr_lin / 100, label=\"SVR Prediction (Linear Kernel)\")\n",
    "plt.title(\"Volatility Prediction using SVR with Linear Kernel\", fontsize=12)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Volatility\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c722c",
   "metadata": {},
   "source": [
    "We repeat the same procedure for the SVR model with the RBF kernel. The RBF kernel is particularly effective for modeling complex nonlinear patterns, which are common in financial volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa834a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rbf = RandomizedSearchCV(svr_rbf, para_grid, n_jobs=-1)\n",
    "clf_rbf.fit(X.iloc[:-n].values, realized_vol.iloc[1:-(n-1)].values.reshape(-1,))\n",
    "predict_svr_rbf = clf_rbf.predict(X.iloc[-n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa759a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_svr_rbf = pd.DataFrame(predict_svr_rbf)\n",
    "predict_svr_rbf.index = ret.iloc[-n:].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b760d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_svr_rbf = np.sqrt(mse(realized_vol.iloc[-n:] / 100, predict_svr_rbf / 100))\n",
    "print('The SVR model with an RBF kernel achieved an RMSE of {:.6f}'.format(rmse_svr_rbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f76793",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(realized_vol / 100, label=\"Realized Volatility\")\n",
    "plt.plot(predict_svr_rbf / 100, label=\"SVR Prediction (RBF Kernel)\")\n",
    "plt.title(\"Volatility Prediction using SVR with RBF Kernel\", fontsize=12)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Volatility\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd01476c",
   "metadata": {},
   "source": [
    "### Results Discussion\n",
    "The SVR model with a linear kernel achieves an RMSE of approximately 0.00143, while the SVR model with the RBF kernel has a higher RMSE of approximately 0.00302. This indicates that, for this dataset, the linear kernel provides more accurate predictions of realized volatility compared to the RBF kernel. The higher RMSE of the RBF kernel suggests that the additional flexibility of the nonlinear kernel may lead to overfitting or less effective generalization in this specific case. The plots confirm this, showing that the linear SVR predictions closely follow the realized volatility, whereas the RBF predictions deviate more from observed values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9dd59",
   "metadata": {},
   "source": [
    "## Neural Networks on GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9191d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "NN_vol = MLPRegressor(learning_rate_init=0.001, random_state=1)\n",
    "para_grid_NN = {'hidden_layer_sizes': [(100, 50), (50, 50), (10, 100)],'max_iter': [500, 1000],'alpha': [0.00005, 0.0005 ]}\n",
    "clf_NN = RandomizedSearchCV(NN_vol, para_grid_NN)\n",
    "clf_NN.fit(X.iloc[:-n].values,realized_vol.iloc[1:-(n-1)].values.reshape(-1, ))\n",
    "NN_predictions = clf_NN.predict(X.iloc[-n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48034349",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_predictions= pd.DataFrame(NN_predictions)\n",
    "NN_predictions.index=ret.iloc[-n:].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_svr_nn = np.sqrt(mse(realized_vol.iloc[-n:] / 100, NN_predictions / 100))\n",
    "print('The RMSE value of Neural Network is {:.6f}'.format(rmse_svr_rbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(realized_vol / 100, label=\"Realized Volatility\")\n",
    "plt.plot(NN_predictions / 100, label=\"Volatility Prediction - SVR\")\n",
    "plt.title(\"Volatility Prediction with SVR\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4e1339",
   "metadata": {},
   "source": [
    "## Deep Learning on GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e22ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.Dense(256, activation=\"relu\"), layers.Dense(128, activation=\"relu\"), layers.Dense(1, activation=\"linear\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4aaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_trial= np.arange(100,400,4)\n",
    "batch_trial= np.arange(100,400, 4)\n",
    "DL_pred=[]\n",
    "DL_RMSE=[]\n",
    "for i, j, k in zip(range(4), epochs_trial, batch_trial):\n",
    "    model.fit(X.iloc[:-n].values, realized_vol.iloc[1:-(n-1)].values.reshape(-1,), batch_size=k, epochs=j, verbose=False)\n",
    "    DL_predict = model.predict(np.asarray(X.iloc[-n:]))\n",
    "    DL_RMSE.append(np.sqrt(mse(realized_vol.iloc[-n:] /100, DL_predict.flatten() /100)))\n",
    "    DL_pred.append(DL_predict)\n",
    "    print(\"DL_RMSE_{}:{:.6f}\".format(i+1, DL_RMSE[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55013fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_predict= pd.DataFrame(DL_pred[DL_RMSE.index(min(DL_RMSE))])\n",
    "DL_predict.index= ret.iloc[-n:].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a44deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(realized_vol / 100, label=\"Realized Volatility\")\n",
    "plt.plot(DL_predict / 100, label=\"Volatility Prediction - Deep Learning\")\n",
    "plt.title(\"Volatility Prediction with Deep Learning using GARCH\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539b725",
   "metadata": {},
   "source": [
    "As far as we tested, based on RMSE, a SVR using a linear kernel function was the best method to predict risk on PayPal stocks. This will be a useful feature to implement to predict stock direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719fcfd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf9d0334",
   "metadata": {},
   "source": [
    "# Predicting stock direction with LINEAR SVR GARCH  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a692f35",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pypl['Predicted_Vol_SVR_Lin'] = predict_svr_lin\n",
    "df_pypl_vol = df_pypl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"open\", \"high\", \"low\", \"close\", \"volume\", \"SMA_20\", \"SMA_50\", \"Predicted_Vol_SVR_Lin\"]\n",
    "X = df_pypl_vol [features]\n",
    "y = df_pypl_vol[\"Target\"]\n",
    "print( \"DataFrame da Paypal com features e Target\")\n",
    "print(df_pypl_vol[features + [\"Target\"]].tail())\n",
    "print(f\"\\nNúmero de amostras prontas para o SVM (X e y): {X.shape[0]}\")\n",
    "print(f\"Número de Features (X): {X.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b73f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, shuffle=False)\n",
    "np.random.seed(42)\n",
    "num_samples= X.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd5f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled= scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
